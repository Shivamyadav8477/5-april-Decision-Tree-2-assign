{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4f399-e051-4550-a7a5-2499180ee8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are a data scientist working for a healthcare company, and you have been tasked with creating a\n",
    "decision tree to help identify patients with diabetes based on a set of clinical variables. You have been\n",
    "given a dataset (diabetes.csv) with the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e64385-22d5-4375-ab91-5307f293fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Pregnancies: Number of times pregnant (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cdf2d-0923-414d-841d-15e2d70736e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "I'll guide you through the process of creating a decision tree for identifying patients with diabetes based on the given dataset. We'll use Python and libraries like pandas, scikit-learn, and matplotlib. Here's a step-by-step approach:\n",
    "\n",
    "Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb260e8-ae94-4996-8175-76dd85a6873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a14a7b-417f-4142-96df-ac4986a344d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23695d3a-644a-4186-bf78-6869e92a7392",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiabetes.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Explore the dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Display the first few rows\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())  # Display the first few rows\n",
    "print(data.info())  # Check data types and missing values\n",
    "print(data.describe())  # Summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab78085-346b-4a89-b70d-f1e2c99a0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a7125-5e79-4565-bd5a-81e8e546a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop('Diabetes', axis=1)\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40c27e-e58b-4065-8c75-af97ec2845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Create and Train the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ddc46-93b5-4e2f-b7da-1064f66a7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e198acb-8113-4a38-a26d-1a01ccb5a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 5: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fa714-48f8-4399-9118-5915c0e72eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f8ac0-177e-49b7-b03a-740b194832fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3a6f8-f08f-4cf5-bdf5-169de116b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d9664-200a-4d26-9df2-1ca62d488526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 7: Visualize the Decision Tree (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42755a9-ab91-4262-bab9-cf2ba15b21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree (you may need to adjust the figure size)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=['No Diabetes', 'Diabetes'], filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3d36b-9a38-48be-8475-69b98d2c8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will load your dataset, preprocess it, create a decision tree classifier, train the model, make predictions, and evaluate its performance. Additionally, it provides an optional step to visualize the decision tree.\n",
    "\n",
    "Make sure to replace 'diabetes.csv' with the actual path to your dataset. You can further fine-tune the model by adjusting hyperparameters and exploring feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9af8b-13c8-4c1b-8300-e1da08e23f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f2ab9-7e6f-4b8e-aab7-4e48b2fc13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great, let's continue building the decision tree model for diabetes prediction using the \"Glucose\" variable. We'll incorporate this variable into the model. Here's how you can do it:\n",
    "\n",
    "Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3962a21-0a1a-426d-8850-30c6ae16f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a952b3b-175d-4b52-bed7-a52e2e304bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7493b3-cb1d-471f-8d9e-58ace523643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())  # Display the first few rows\n",
    "print(data.info())  # Check data types and missing values\n",
    "print(data.describe())  # Summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16ea3f-358a-402c-9e5a-fab2eedb99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Preprocess the Data\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf9537-fd78-4006-b71b-665ecc651c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data[['Pregnancies', 'Glucose']]  # Include the \"Glucose\" variable\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d619a-6714-4414-83d0-2caabb95dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Create and Train the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a760f4-3b4a-4165-aa92-f86a0b75ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d18903-5892-43d6-bdcc-05cff25fca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 5: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf6a18-e4e4-4e80-bee4-3429f177026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6ef29-6cc2-4543-814d-a2607fb82e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de01e1f-1971-4f4d-b675-a95b337b1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a5df7-b668-4310-a596-49aa104fddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep 7: Visualize the Decision Tree (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba20fb8-44c9-4f22-b303-52f9fc355dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree (you may need to adjust the figure size)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, feature_names=['Pregnancies', 'Glucose'], class_names=['No Diabetes', 'Diabetes'], filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f1d39-4ff5-48a6-bcbb-8c56b3573b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code extends the previous example by including the \"Glucose\" variable in the feature set and adjusting the corresponding steps accordingly. You can continue to add more relevant features to improve the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b58e8-3c95-4107-878c-27104e8cbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. BloodPressure: Diastolic blood pressure (mm Hg) (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbfbb-7fff-417a-8af0-0a11e4335945",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly, we'll incorporate the \"BloodPressure\" variable into the decision tree model for diabetes prediction. Here are the steps to do that:\n",
    "\n",
    "Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af4bd3-3599-4ca2-ac91-3354b0fecbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59914766-52dc-4c86-9f79-83b9592488cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd7777-11a1-446d-8b04-d1da342c0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())  # Display the first few rows\n",
    "print(data.info())  # Check data types and missing values\n",
    "print(data.describe())  # Summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37c592-350f-41da-9aad-945babb91543",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26abaec-412c-4b56-9140-4b4153e361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data[['Pregnancies', 'Glucose', 'BloodPressure']]  # Include the \"Glucose\" and \"BloodPressure\" variables\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acfb4f-6894-4ff3-99e0-69da16947c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Create and Train the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cc58c-985b-4b75-b525-afc10875bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390241b-4614-49ea-a50a-a7d39ccd7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 5: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f9b86-4a71-409e-b0cb-fc8d56335454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41df2ca-cbe1-4c45-90cb-532b07947042",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece76193-9b05-4ee1-8dd7-42f7f691749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8f20e-9ccf-4007-964b-58ef5ac58a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 7: Visualize the Decision Tree (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b5d0a-05e4-4a45-b91e-a34d858b73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree (you may need to adjust the figure size)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, feature_names=['Pregnancies', 'Glucose', 'BloodPressure'], class_names=['No Diabetes', 'Diabetes'], filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409112f-99ca-405b-8ef7-95d6842aed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code extends the previous example by including the \"BloodPressure\" variable in the feature set and adjusting the corresponding steps accordingly. You can continue to add more relevant features to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f040217-48b8-481b-9f95-8091c8562169",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to\n",
    "understand the distribution and relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714c115-3512-4c71-82a0-8a3a007bca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "To import the dataset, examine the variables, and understand the distribution and relationships between the variables, you can follow these steps in Python using popular libraries like pandas, matplotlib, and seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e25c6-7f53-400c-8401-417387d4ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset (assuming the dataset is named \"diabetes.csv\")\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Examine the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Visualize the distribution of variables\n",
    "# You can create histograms for numerical variables\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(data['Pregnancies'], kde=True)\n",
    "plt.title('Distribution of Pregnancies')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(data['Glucose'], kde=True)\n",
    "plt.title('Distribution of Glucose')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(data['BloodPressure'], kde=True)\n",
    "plt.title('Distribution of BloodPressure')\n",
    "\n",
    "# Create a pairplot to visualize relationships between numerical variables\n",
    "sns.pairplot(data, hue='Diabetes', diag_kind='kde')\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation heatmap for numerical variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02e73b-ac80-4599-b366-9b6aa5425af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will load the dataset, display the first few rows, provide summary statistics, and create visualizations to explore the distribution of numerical variables and relationships between them. Adjust the code as needed based on your dataset's specific column names and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471f25e-b3b2-496f-875a-90d77f9bc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical\n",
    "variables into dummy variables if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7277ea9-136b-423b-98eb-6271645b52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the data, including handling missing values, removing outliers, and transforming categorical variables into dummy variables if necessary, you can use the following Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccedb4-6ce0-447b-8003-e0f4d083f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (assuming the dataset is named \"diabetes.csv\")\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Handling Missing Values (Assuming missing values are represented as 0 in the dataset)\n",
    "# Replace 0 values in relevant columns (e.g., Glucose, BloodPressure) with NaN\n",
    "cols_with_zeros = ['Glucose', 'BloodPressure', 'BMI', 'Insulin', 'SkinThickness']\n",
    "data[cols_with_zeros] = data[cols_with_zeros].replace(0, np.nan)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Impute missing values (you can use mean, median, or other strategies)\n",
    "data['Glucose'].fillna(data['Glucose'].mean(), inplace=True)\n",
    "data['BloodPressure'].fillna(data['BloodPressure'].mean(), inplace=True)\n",
    "data['BMI'].fillna(data['BMI'].mean(), inplace=True)\n",
    "data['Insulin'].fillna(data['Insulin'].median(), inplace=True)\n",
    "data['SkinThickness'].fillna(data['SkinThickness'].median(), inplace=True)\n",
    "\n",
    "# Removing Outliers (optional)\n",
    "# You can define your criteria for outliers and remove them\n",
    "# For example, remove values that are more than 3 standard deviations away from the mean\n",
    "z_scores = (data[['Glucose', 'BloodPressure', 'BMI', 'Insulin', 'SkinThickness']] - data[['Glucose', 'BloodPressure', 'BMI', 'Insulin', 'SkinThickness']].mean()) / data[['Glucose', 'BloodPressure', 'BMI', 'Insulin', 'SkinThickness']].std()\n",
    "data = data[(z_scores.abs() < 3).all(axis=1)]\n",
    "\n",
    "# Transform Categorical Variables into Dummy Variables (if needed)\n",
    "# If you have categorical variables, you can convert them to dummy variables\n",
    "# Example: data = pd.get_dummies(data, columns=['CategoricalColumnName'])\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop('Diabetes', axis=1)\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Now, the data is preprocessed and ready for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27613ead-fbd0-4081-8257-555ce9779998",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6af0a-9bb2-4a30-a364-93568c51165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the previous code snippet, I've already split the dataset into a training set (X_train, y_train) and a test set (X_test, y_test) using the train_test_split function. I also set a random seed (random_state=42) to ensure reproducibility. Here's the relevant part of the code for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6310e1-1bf6-465a-8c60-9e25ff15c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3c7c0-6c39-4f14-ab9c-a66bf3700593",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code randomly splits the dataset into training (80%) and testing (20%) sets while maintaining reproducibility due to the specified random seed (42). You can use X_train, X_test, y_train, and y_test for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d286951-fbc5-4928-be8e-8acefe8ffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use\n",
    "cross-validation to optimize the hyperparameters and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3271a-0f21-4c52-b720-ad8a9a769809",
   "metadata": {},
   "outputs": [],
   "source": [
    "To train a decision tree model using the training set and optimize hyperparameters while avoiding overfitting, you can use scikit-learn, a popular Python library for machine learning. Here's an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f249fe-fb81-4bdf-974c-2eefaa99f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Create a DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters and their possible values for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the decision tree model with the best hyperparameters\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model using cross-validation to assess its performance\n",
    "cross_val_scores = cross_val_score(best_decision_tree, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cross_val_scores)\n",
    "print(\"Mean CV Accuracy:\", cross_val_scores.mean())\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "best_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Now you have a trained decision tree model with optimized hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce3e73-3cc7-4a68-8199-adf367e15801",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this code:\n",
    "\n",
    "We create a DecisionTreeClassifier and define a grid of hyperparameters (max_depth, min_samples_split, and min_samples_leaf) with possible values to search over.\n",
    "\n",
    "We perform a grid search cross-validation using GridSearchCV to find the best hyperparameters that optimize model performance.\n",
    "\n",
    "We train the decision tree model using the best hyperparameters found during cross-validation.\n",
    "\n",
    "We evaluate the model's performance using cross-validation to ensure it's not overfitting. The mean cross-validation accuracy is printed.\n",
    "\n",
    "Finally, we fit the model on the entire training set using the best hyperparameters.\n",
    "\n",
    "This process helps you train a decision tree model with optimal hyperparameters and assess its performance on the training data. Adjust the hyperparameter grid and scoring metric as needed for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b833d3-8c06-44f0-8a8e-6d2229b89480",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy,\n",
    "precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5fda9-259f-4a48-b4cd-6c32d42a6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "To evaluate the performance of the decision tree model on the test set and calculate metrics such as accuracy, precision, recall, and F1 score, as well as visualize the results using confusion matrices and ROC curves, you can use scikit-learn. Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c4ca2-347a-4240-80e7-16e7b3aeba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_decision_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Generate and plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "classes = [0, 1]  # Assuming binary classification\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot the ROC curve\n",
    "y_scores = best_decision_tree.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a10d-a0aa-4d43-b3a6-fd2be7ecc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this code:\n",
    "\n",
    "We make predictions on the test set using the trained decision tree model.\n",
    "\n",
    "We calculate accuracy, precision, recall, and F1 score using scikit-learn's metrics functions.\n",
    "\n",
    "We generate and plot the confusion matrix to visualize true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "We generate and plot the ROC curve to visualize the model's ability to distinguish between classes. The AUC (Area Under the Curve) is also calculated.\n",
    "\n",
    "This code provides a comprehensive evaluation of the decision tree model's performance on the test set and visualizes the results using confusion matrices and ROC curves. Adjust the class labels (0 and 1) and scoring metrics as needed for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e97b0-4fd9-4f3d-a5e9-e6e3ec7f2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important\n",
    "variables and their thresholds. Use domain knowledge and common sense to explain the patterns and\n",
    "trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26a10d-3afb-49d7-ba34-2088ddb9205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpreting a decision tree involves examining the splits, branches, and leaves to understand how the model makes predictions. Let's break down the interpretation of a decision tree for a diabetes classification problem, identifying the most important variables and their thresholds:\n",
    "\n",
    "Root Node: The first split in the decision tree, often referred to as the root node, represents the initial decision point. In this case, the root node might be based on a feature that has the highest information gain or Gini impurity. Let's say the root node splits the data based on the \"Glucose\" feature.\n",
    "\n",
    "Internal Nodes: As we move down the tree, we encounter internal nodes representing further splits. For instance, the \"Glucose\" feature might split into \"Glucose <= 120\" and \"Glucose > 120\" at a certain threshold.\n",
    "\n",
    "Leaf Nodes: The terminal nodes or leaf nodes are where the decision tree provides a final prediction. Each leaf node corresponds to a class label (e.g., \"Diabetes\" or \"No Diabetes\"). The majority class in a leaf node is the predicted class.\n",
    "\n",
    "Thresholds: Thresholds are values in the feature space that determine how data points are partitioned at each split. For example, a threshold of \"Glucose <= 120\" means that patients with a glucose level less than or equal to 120 mg/dL follow one branch, while those with glucose levels greater than 120 mg/dL follow another branch.\n",
    "\n",
    "Importance of Variables: The importance of a variable in a decision tree can be measured by how often it is used for splitting and how much it reduces impurity or increases information gain. Variables used near the top of the tree, particularly at the root node and first few splits, tend to be more important.\n",
    "\n",
    "Patterns and Trends: To interpret the decision tree, you should examine the patterns and trends in the splits. For example, if \"Glucose\" is a significant predictor at the root node, it suggests that glucose levels have a substantial impact on diabetes prediction. Thresholds indicate specific glucose levels that are associated with different risk levels.\n",
    "\n",
    "Domain Knowledge: Incorporating domain knowledge and common sense is crucial for a meaningful interpretation. You can relate findings to existing medical knowledge, guidelines, or clinical thresholds. For instance, if the tree splits on \"Glucose <= 120,\" you can check if 120 mg/dL aligns with any clinical guidelines for glucose management.\n",
    "\n",
    "Pruning: Decision trees can be deep and complex, potentially overfitting the data. Pruning techniques can simplify the tree while preserving important splits. This can lead to more interpretable and generalizable models.\n",
    "\n",
    "Visualization: Visualizing the decision tree using libraries like Graphviz can help in understanding its structure. It displays the tree graphically, making it easier to see the splits and branches.\n",
    "\n",
    "Remember that decision trees are interpretable models by nature, and their transparency makes them valuable for understanding the relationships between features and outcomes. Interpretation should involve both a technical understanding of the tree structure and a domain-specific understanding of the variables and thresholds used in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bd508-02c6-43e2-bd80-7d1852351e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53574b61-9bb2-4491-872d-92ce15792adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c6b96-075e-4dff-8416-ca332d7b3fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
